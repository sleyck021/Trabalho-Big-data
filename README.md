markdown
# ğŸš€ Projeto de CiÃªncia de Dados e Big Data â€“ Pipeline de Vendas

Este projeto implementa um pipeline completo de **CiÃªncia de Dados/Big Data**, cobrindo ingestÃ£o, processamento, armazenamento em Data Lake e visualizaÃ§Ã£o de KPIs.  
A soluÃ§Ã£o foi construÃ­da com **Airflow, Spark, MinIO e Metabase**, refletindo prÃ¡ticas reais de engenharia de dados.

---

## ğŸ“Œ Objetivo

- Demonstrar domÃ­nio tÃ©cnico em coleta, processamento e anÃ¡lise de dados.
- Estruturar um fluxo robusto com camadas **Raw â†’ Bronze â†’ Silver â†’ Gold**.
- Disponibilizar insights via dashboards e API.
- Garantir documentaÃ§Ã£o clara e repositÃ³rio organizado.

---

## ğŸ“‚ Estrutura do RepositÃ³rio

â”œâ”€â”€ README.md # Guia geral do projeto
â”œâ”€â”€ docs/ # DocumentaÃ§Ã£o detalhada
â”‚ â”œâ”€â”€ arquitetura.md
â”‚ â”œâ”€â”€ dados.md
â”‚ â”œâ”€â”€ execucao.md
â”‚ â””â”€â”€ decisoes-tecnicas.md
â”œâ”€â”€ infra/ # Infraestrutura (Docker, configs)
â”‚ â”œâ”€â”€ docker-compose.yml
â”‚ â”œâ”€â”€ minio/init.sh
â”‚ â”œâ”€â”€ airflow/Dockerfile
â”‚ â”œâ”€â”€ airflow/requirements.txt
â”‚ â””â”€â”€ spark/spark-defaults.conf
â”œâ”€â”€ src/ # CÃ³digo-fonte
â”‚ â”œâ”€â”€ ingestion/ingest_to_minio.py
â”‚ â”œâ”€â”€ processing/spark_job.py
â”‚ â””â”€â”€ api/main.py
â”œâ”€â”€ notebooks/ # AnÃ¡lises exploratÃ³rias
â”‚ â””â”€â”€ 01_exploracao.ipynb
â””â”€â”€ vendas.csv
---

## âš™ï¸ Tecnologias Utilizadas

- **Airflow** â†’ OrquestraÃ§Ã£o de tarefas
- **Spark** â†’ Processamento distribuÃ­do
- **MinIO** â†’ Data Lake compatÃ­vel com S3
- **Metabase** â†’ VisualizaÃ§Ã£o de KPIs
- **FastAPI** â†’ API opcional para servir dados
- **Docker Compose** â†’ Infraestrutura containerizada

---

## ğŸ—‚ï¸ Camadas do Data Lake

- **Raw:** dados brutos (CSV/Parquet)
- **Bronze:** schema e tipagem padronizados
- **Silver:** dados limpos e enriquecidos
- **Gold:** agregaÃ§Ãµes e KPIs prontos para anÃ¡lise

---

## ğŸš€ Passo a Passo de ExecuÃ§Ã£o

### 1. Subir a infraestrutura
```bash
cd infra
docker compose up -d --build
MinIO: http://localhost:9001 (user: minio, pass: minio123)

Airflow: http://localhost:8080 (user: admin, pass: admin)

Metabase: http://localhost:3000

Spark: porta 7077

2. IngestÃ£o de dados (Raw)
bash
docker exec -it airflow bash -lc "python /opt/project/src/ingestion/ingest_to_minio.py"
Resultado: arquivos vendas.csv e vendas.parquet em s3://raw/vendas/.

3. Processamento (Bronze â†’ Silver â†’ Gold)
bash
docker exec -it spark bash -lc "spark-submit /opt/project/src/processing/spark_job.py"
Resultado:

s3://bronze/vendas

s3://silver/vendas

s3://gold/vendas_daily_city

s3://gold/vendas_by_product

4. OrquestraÃ§Ã£o com Airflow
Acesse http://localhost:8080

Ative o DAG pipeline_vendas

Execute manualmente ou agende para rodar diariamente

Verifique logs e status das tarefas

5. VisualizaÃ§Ã£o com Metabase
Acesse http://localhost:3000

Configure conexÃ£o (Postgres interno ou exporte CSV do Gold)

Crie dashboards com KPIs:

Receita por cidade/data

Receita por produto

Unidades vendidas por regiÃ£o

6. API opcional (FastAPI)
bash
export MINIO_ENDPOINT=localhost:9000 MINIO_ACCESS_KEY=minio MINIO_SECRET_KEY=minio123
uvicorn src.api.main:app --reload --port 8000
Endpoints disponÃ­veis:

GET /v1/revenue/city â†’ Top 20 cidades por receita

GET /v1/revenue/product â†’ Top 20 produtos por receita

ğŸ“Š Dataset de Exemplo
Arquivo: datasets/vendas.csv

Campos:

order_id â†’ ID do pedido

order_date â†’ Data do pedido

customer_id â†’ Cliente

store_id â†’ Loja

product_id â†’ Produto

quantity â†’ Quantidade

unit_price â†’ PreÃ§o unitÃ¡rio

currency â†’ Moeda

city â†’ Cidade

state â†’ Estado

ğŸ” CritÃ©rios de AvaliaÃ§Ã£o
Entendimento da soluÃ§Ã£o entregue

Clareza sobre papel individual

NoÃ§Ãµes de arquitetura de dados

DomÃ­nio das ferramentas utilizadas

DocumentaÃ§Ã£o completa e organizada

âš ï¸ LimitaÃ§Ãµes e Melhorias Futuras
Metabase nÃ£o lÃª Parquet direto do MinIO â†’ soluÃ§Ã£o: materializar em Postgres

ValidaÃ§Ãµes bÃ¡sicas â†’ pode evoluir para Great Expectations

Sem catÃ¡logo formal â†’ pode ser adicionado Glue/Unity Catalog

Escalabilidade â†’ adicionar mais workers Spark/Airflow

ğŸ“ Como reiniciar do zero
bash
docker compose down -v
docker compose up -d --build